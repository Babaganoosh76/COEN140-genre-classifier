{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: LDA and QDA Classifier with SKLearn\n",
    "In this lab, I'll use the SKLearn toolkit to implement a Linear Discriminant Analysis Classifier and a Quadratic Discriminant Analysis Classifier. The train error and test errors are printed below. I will also run additional tests to find which variables are least important in classifying the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">========= LDA Results ========<\n",
      "Training error: 0.00%\n",
      "Testing error: 2.50%\n",
      "\n",
      ">========= QDA Results ========<\n",
      "Training error: 0.00%\n",
      "Testing error: 1.67%\n",
      "\n",
      "Training error while excluding variable 1: 2.50%\n",
      "Testing error while excluding variable 1: 0.00%\n",
      "Training error while excluding variable 2: 1.67%\n",
      "Testing error while excluding variable 2: 0.00%\n",
      "Training error while excluding variable 3: 4.17%\n",
      "Testing error while excluding variable 3: 0.00%\n",
      "Training error while excluding variable 4: 5.83%\n",
      "Testing error while excluding variable 4: 3.33%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "'''\n",
    "Much help:\n",
    "https://stackabuse.com/implementing-lda-in-python-with-scikit-learn/\n",
    "'''\n",
    "\n",
    "def test_dataset(data):\n",
    "    if len(data) != 150:\n",
    "        print(1)\n",
    "        return False\n",
    "    \n",
    "    for row in data:\n",
    "        if len(row) != 5:\n",
    "            print('len', len(row))\n",
    "            return False\n",
    "        \n",
    "        for column in row[:-1]:\n",
    "            if type(column) != np.float64:\n",
    "                print(type(column))\n",
    "                return False\n",
    "            \n",
    "        if type(row[-1]) != str:\n",
    "            print(4)\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def read_data():\n",
    "    data = pd.read_csv('iris.data', names=['sl','sw','pl','pw','class'], dtype={'sl':np.float64,'sw':np.float64,'pl':np.float64,'pw':np.float64,'class':str})\n",
    "    return data\n",
    "\n",
    "def train_test_split(data, x_col, y_col):\n",
    "    x_all = data.iloc[:,:x_col] if isinstance(x_col, int) else data.iloc[:,x_col]\n",
    "    y_all = data.iloc[:,y_col]\n",
    "    X_train = pd.concat([x_all.iloc[0:40], x_all.iloc[50:90], x_all.iloc[100:140]])\n",
    "    X_test = pd.concat([x_all.iloc[40:50], x_all.iloc[90:100], x_all.iloc[140:150]])\n",
    "    y_train = pd.concat([y_all.iloc[0:40], y_all.iloc[50:90], y_all.iloc[100:140]])\n",
    "    y_test = pd.concat([y_all.iloc[40:50], y_all.iloc[90:100], y_all.iloc[140:150]])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def lda_classifier(X_train, y_train):\n",
    "    # create LDA Classifier\n",
    "    lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    \n",
    "    # train over training data\n",
    "    lda.fit(X_train, y_train)\n",
    "    \n",
    "    # return trained classifier\n",
    "    return lda\n",
    "\n",
    "def qda_classifier(X_train, y_train):\n",
    "    # create QDA Classifier\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    \n",
    "    # train over training data\n",
    "    qda.fit(X_train, y_train)\n",
    "    \n",
    "    # return trained classifier\n",
    "    return qda\n",
    "\n",
    "def prediction_results(pred, test):\n",
    "    n = len(test)\n",
    "    assert len(pred) == n\n",
    "    correct = 0\n",
    "    for i in range(n):\n",
    "        if (pred[i] == test[i]):\n",
    "            correct = correct + 1\n",
    "    return (1-correct/n)*100\n",
    "\n",
    "def main():\n",
    "\n",
    "    # get data as dataframe\n",
    "    data = read_data()\n",
    "\n",
    "    # create test and training sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, 4, 4)\n",
    "\n",
    "    # create classifier\n",
    "    lda = lda_classifier(X_train.values, y_train.values)\n",
    "    qda = qda_classifier(X_train.values, y_train.values)\n",
    "\n",
    "    # predict outcomes for test data\n",
    "    lda_train_pred = lda.predict(X_test.values)\n",
    "    lda_test_pred = lda.predict(X_train.values)\n",
    "    qda_train_pred = qda.predict(X_test.values)\n",
    "    qda_test_pred = qda.predict(X_train.values)\n",
    "    \n",
    "    print('>========= LDA Results ========<')\n",
    "    p1 = prediction_results(lda_train_pred, y_test.values)\n",
    "    p2 = prediction_results(lda_test_pred, y_train.values)\n",
    "    print('Training error: {:.2f}%\\nTesting error: {:.2f}%\\n'.format(p1, p2))\n",
    "\n",
    "    print('>========= QDA Results ========<')\n",
    "    p3 = prediction_results(qda_train_pred, y_test.values)\n",
    "    p4 = prediction_results(qda_test_pred, y_train.values)\n",
    "    print('Training error: {:.2f}%\\nTesting error: {:.2f}%\\n'.format(p3, p4))\n",
    "    \n",
    "    # test excluding each variable\n",
    "    rng = [0,1,2,3]\n",
    "    pr = []\n",
    "    for i in range(4):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, rng[:i]+rng[i+1:], 4)\n",
    "        clfr = qda_classifier(X_train.values, y_train.values)\n",
    "        pred = (clfr.predict(X_train.values), clfr.predict(X_test.values))\n",
    "        err = (prediction_results(pred[0], y_train.values), prediction_results(pred[1], y_test.values))\n",
    "        print('Training error while excluding variable {}: {:.2f}%'.format(i+1,err[0]))\n",
    "        print('Testing error while excluding variable {}: {:.2f}%'.format(i+1,err[1])) \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my testing, I've concluded that sepal length and sepal width individually non-important to classification, since their exclusion only caused a minimal training error and a 0% testing error. Petal width is the most important variable, since the training error spiked up to 5.83% and the testing error to 3.33% when it was excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
